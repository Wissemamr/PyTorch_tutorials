{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01 : PyTorch workflow\n",
    "\n",
    "\n",
    "1. Create a straight line dataset using the linear regression formula (weight * X + bias).\n",
    "- Set weight=0.3 and bias=0.9 there should be at least 100 datapoints total.\n",
    "- Split the data into 80% training, 20% testing.\n",
    "- Plot the training and testing data so it becomes visual.\n",
    "2. Build a PyTorch model by subclassing nn.Module.\n",
    "- Inside should be a randomly initialized nn.Parameter() with requires_grad=True, one for weights and one for bias.\n",
    "- Implement the forward() method to compute the linear regression function you used to create the dataset in 1.\n",
    "Once you've constructed the model, make an instance of it and check its state_dict().\n",
    "- Note: If you'd like to use nn.Linear() instead of nn.Parameter() you can.\n",
    "3. Create a loss function and optimizer using nn.L1Loss() and torch.optim.SGD(params, lr) respectively.\n",
    "- Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.\n",
    "- Write a training loop to perform the appropriate training steps for 300 epochs.\n",
    "- The training loop should test the model on the test dataset every 20 epochs.\n",
    "4. Make predictions with the trained model on the test data.\n",
    "- Visualize these predictions against the original training and testing data \n",
    "(note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).\n",
    "5. Save your trained model's state_dict() to file.\n",
    "- Create a new instance of your model class you made in 2. and load in the state_dict() you just saved to it.\n",
    "Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 data points\n"
     ]
    }
   ],
   "source": [
    "#  Set weight=0.3 and bias=0.9 there should be at least 100 datapoints total.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "DEBUG: bool = True\n",
    "weight = 0.3\n",
    "bias = 0.9\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.01\n",
    "\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "if DEBUG:\n",
    "    print(f\"There are {len(X)} data points\")\n",
    "y = X * weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 1]) torch.Size([80, 1])\n",
      "torch.Size([20, 1]) torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 80% training, 20% testing.\n",
    "train_split_idx = int((80 * len(X)) / 100)\n",
    "X_train, y_train = X[:train_split_idx], y[:train_split_idx]\n",
    "if DEBUG:\n",
    "    print(X_train.shape, y_train.shape)\n",
    "X_test, y_test = X[train_split_idx:], y[train_split_idx:]\n",
    "if DEBUG:\n",
    "    print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot the training and testing data so it becomes visual.\n",
    "\n",
    "\n",
    "def plot_predictions(\n",
    "    train_data=X_train,\n",
    "    train_labels=y_train,\n",
    "    test_data=X_test,\n",
    "    test_labels=y_test,\n",
    "    predictions=None,\n",
    "):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.scatter(train_data, train_labels, c=\"magenta\", s=5, label=\"Training data\")\n",
    "    plt.scatter(test_data, test_labels, c=\"blue\", s=5, label=\"Testing data\")\n",
    "\n",
    "    # plot predictions\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data, predictions, c=\"green\", s=5, label=\"Predictions\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside should be a randomly initialized nn.Parameter() with requires_grad=True, one for weights and one for bias.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class WLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(1, dtype=torch.float32), requires_grad=True\n",
    "        )\n",
    "        \n",
    "        self.biais = nn.Parameter(\n",
    "            torch.randn(1, dtype=torch.float32), requires_grad=True\n",
    "        )\n",
    "\n",
    "    # ALTERNATIVE with nn.Linear\n",
    "\n",
    "    # Implement the forward() method to compute the linear regression function you used to create the dataset in 1.\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.weights *x  + self.biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weights', tensor([0.3367])), ('biais', tensor([0.1288]))])\n"
     ]
    }
   ],
   "source": [
    "w_model = WLinearRegressionModel()\n",
    "print(w_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC CABA DZ\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Create a l1 loss function and optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params = w_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 1\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 2\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 3\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 4\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 5\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 6\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 7\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 8\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 9\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 10\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 11\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 12\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 13\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 14\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 15\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 16\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 17\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 18\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 19\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 20\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 21\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 22\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 23\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 24\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 25\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 26\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 27\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 28\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 29\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 30\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 31\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 32\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 33\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 34\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 35\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 36\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 37\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 38\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 39\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 40\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 41\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 42\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 43\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 44\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 45\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 46\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 47\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 48\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 49\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 50\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 51\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 52\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 53\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 54\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 55\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 56\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 57\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 58\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 59\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 60\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 61\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 62\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 63\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 64\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 65\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 66\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 67\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 68\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 69\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 70\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 71\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 72\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 73\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 74\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 75\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 76\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 77\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 78\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 79\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 80\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 81\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 82\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 83\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 84\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 85\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 86\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 87\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 88\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 89\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 90\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 91\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 92\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 93\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 94\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 95\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 96\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 97\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 98\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 99\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 100\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 101\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 102\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 103\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 104\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 105\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 106\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 107\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 108\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 109\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 110\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 111\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 112\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 113\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 114\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 115\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 116\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 117\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 118\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 119\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 120\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 121\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 122\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 123\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 124\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 125\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 126\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 127\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 128\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 129\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 130\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 131\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 132\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 133\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 134\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 135\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 136\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 137\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 138\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 139\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 140\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 141\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 142\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 143\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 144\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 145\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 146\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 147\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 148\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 149\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 150\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 151\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 152\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 153\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 154\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 155\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 156\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 157\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 158\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 159\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 160\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 161\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 162\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 163\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 164\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 165\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 166\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 167\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 168\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 169\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 170\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 171\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 172\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 173\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 174\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 175\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 176\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 177\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 178\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 179\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 180\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 181\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 182\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 183\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 184\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 185\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 186\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 187\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 188\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 189\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 190\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 191\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 192\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 193\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 194\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 195\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 196\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 197\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 198\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 199\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 200\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 201\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 202\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 203\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 204\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 205\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 206\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 207\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 208\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 209\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 210\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 211\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 212\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 213\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 214\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 215\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 216\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 217\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 218\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 219\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 220\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 221\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 222\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 223\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 224\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 225\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 226\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 227\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 228\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 229\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 230\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 231\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 232\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 233\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 234\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 235\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 236\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 237\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 238\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 239\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 240\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 241\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 242\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 243\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 244\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 245\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 246\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 247\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 248\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 249\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 250\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 251\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 252\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 253\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 254\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 255\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 256\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 257\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 258\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 259\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 260\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 261\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 262\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 263\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 264\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 265\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 266\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 267\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 268\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 269\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 270\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 271\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 272\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 273\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 274\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 275\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 276\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 277\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 278\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 279\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 280\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 281\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 282\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 283\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 284\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 285\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 286\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 287\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 288\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 289\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 290\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 291\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 292\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 293\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 294\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 295\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 296\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 297\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n",
      "Epoch : 298\n",
      "\tMAE Train Loss (L1 Loss) --> 0.007858465425670147\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.007045787759125233 \n",
      "\n",
      "Epoch : 299\n",
      "\tMAE Train Loss (L1 Loss) --> 0.00370177929289639\n",
      "\tMAE Test Loss (L1 Loss)  --> 0.006489443592727184 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write the training loop for 300 epochs\n",
    "epochs = 300\n",
    "\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "epoch_counts =[]\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "        w_model.train()\n",
    "        y_pred = w_model(X_train)\n",
    "        train_loss = loss_fn(y_pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        w_model.eval()\n",
    "        with torch.inference_mode():\n",
    "            test_pred = w_model(X_test)\n",
    "            test_loss = loss_fn(test_pred, y_test.type(torch.float))\n",
    "            epoch_counts.append(epoch_counts)\n",
    "            training_losses.append(train_loss.detach().numpy())\n",
    "            testing_losses.append(test_loss.detach().numpy())\n",
    "            print(\n",
    "            f\"Epoch : {epoch}\\n\\tMAE Train Loss (L1 Loss) --> {train_loss}\\n\\tMAE Test Loss (L1 Loss)  --> {test_loss} \\n\"\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3067], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9011], requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1464],\n",
      "        [1.1495],\n",
      "        [1.1525],\n",
      "        [1.1556],\n",
      "        [1.1587],\n",
      "        [1.1617],\n",
      "        [1.1648],\n",
      "        [1.1679],\n",
      "        [1.1709],\n",
      "        [1.1740],\n",
      "        [1.1771],\n",
      "        [1.1801],\n",
      "        [1.1832],\n",
      "        [1.1863],\n",
      "        [1.1893],\n",
      "        [1.1924],\n",
      "        [1.1955],\n",
      "        [1.1985],\n",
      "        [1.2016],\n",
      "        [1.2047]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# make prediciton son the test data\n",
    "w_model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = w_model(X_test)\n",
    "    print(y_preds)\n",
    "    print(type(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weights', tensor([0.3067])), ('biais', tensor([0.9011]))])\n"
     ]
    }
   ],
   "source": [
    "print(w_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to : my_models\\00.pt_linear_regression_model_exercise.pth\n",
      "Model saved successfully !\n"
     ]
    }
   ],
   "source": [
    "# save the trained model state dict\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_NAME = \"00.pt_linear_regression_model_exercise.pth\"\n",
    "MODEL_SAVE_PATH = Path('my_models')/ MODEL_NAME\n",
    "\n",
    "# Save the modelstate dict\n",
    "print(f\"Saving model to : {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=w_model.state_dict(), f=MODEL_SAVE_PATH)\n",
    "print(\"Model saved successfully !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "loaded_w_model = WLinearRegressionModel()\n",
    "loaded_w_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "loaded_w_model.eval()\n",
    "with torch.inference_mode():\n",
    "    loaded_y_preds = loaded_w_model(X_test)\n",
    "    print(y_preds==loaded_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WLinearRegressionModel()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
